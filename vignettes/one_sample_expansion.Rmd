---
title: "One-sample expansion cohort"
author: "Emily C. Zabor"
date: "Last updated: `r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{One-sample expansion cohort}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


# Introduction

The goal of this article is to provide a tutorial on the use of the `calc_posterior()` and `calc_predictive()` functions in the context of a one-sample expansion cohort. It is becoming increasingly common for expansion cohorts to be included at the end of phase I trials. An expansion cohort is a group of additional patients being treated at the recommended phase 2 dose (RP2D) that was identified in the dose finding portion of the phase I study, and is typically used to further characterize the toxicity and/or efficacy of the RP2D. Since one goal of expansion cohorts is to conserve the limited available resources for clinical studies, early stopping for futility is an important consideration. In this article we demonstrate the use of sequential posterior probability monitoring for futility in the context of a one-sample study.


# Setup

To execute the code in this tutorial, you first need to install and load the {ppseq} package.

```{r setup}
# remotes::install_github("zabore/ppseq")
library(ppseq)
```


# Methods

We want to plan a one-sample study. The unacceptable response rate is 0.1, and the acceptable response rate 0.3. We want to look after every 5 patients, with a maximum sample size of 50.

Let's start by seeing what happens in a single simulated trial. We use the function `sim_single_trial` to simulate response counts based on the properties of our trial and calculate the posterior predictive value of exceeding the target posterior probability of success.

We would stop for futility if our posterior predictive probability of exceeding the target posterior probability, which in this case is 0.95, falls below 0.05. 

First let's consider a trial where we achieve our acceptable response rate.

```{r}
sim_single_trial(prob = 0.3, n = seq(5, 50, 5), direction = "greater", p = 0.1,
                 delta = NULL, prior = c(0.5,0.5), S = 5000, seed = 1, 
                 N = 50, theta = 0.95)
```

We see here that the posterior predictive probability, in the column `pp`, never drops below the futility level of 0.05, so we would continue enrolling in this expansion cohort until the end of the trial, where we see we have reached a 100\% predictive probability of having posterior probability exceeding 0.95. If we had set a stopping rule for efficacy, we could have stopped after 20 patients when the posterior predictive probability reached 0.9972, which would exceed an efficacy stopping rule of 0.95.

Now let's consider a trial where we do not achieve our acceptable response rate, but rather see the unacceptable response rate.

```{r}
sim_single_trial(prob = 0.1, n = seq(5, 50, 5), direction = "greater", p = 0.1,
                 delta = NULL, prior = c(0.5,0.5), S = 5000, seed = 1, 
                 N = 50, theta = 0.95)
```

We see in this example that we drop below the futility threshold of 0.05 after enrolling just 10 patients, after which we have a posterior predictive probability of 0.0390. This would cause the trial to stop for futility to preserve resources for the investigation of more promising treatments.

But when we are designing a trial, what we want is to know the operating characteristics across many simulated trials.

```{r eval = FALSE}
# This is just me trying to figure out Brian's code from the Covid trial!
# Still not sure what comes next.

 # prob y1 < y0
 posterior_y0.gt.y1 <- function(n0, n1, y0, y1, delta=0, prior=c(0.5,0.5)){ ## y0 standard     y1 experimental
  SAMP <- 10000
  out <- sum( rbeta(SAMP, prior[1] + y0, prior[2] + n0 - y0) > ( rbeta(SAMP, prior[1] + y1, prior[2] + n1 - y1) + delta ) ) / SAMP
  return(out)
 }

find.Boundary <- function(j, theta, Tab, delta=0, prior=c(0.5,0.5) ){

 Y=Tab$n1[j]; STOP <- FALSE
 temp <- posterior_y0.gt.y1( n0=Tab$n0[j], n1=Tab$n1[j], y0=Tab$y0[j], y1=Y, delta=delta, prior=prior )
 if( is.na(temp) ){ temp <- 0; print(paste0("NA - ",Tab[j,])) }
 if( temp > theta ){ STOP <- TRUE }
 while( !STOP ){
  Y <- Y - 1
  temp <- posterior_y0.gt.y1( n0=Tab$n0[j], n1=Tab$n1[j], y0=Tab$y0[j], y1=Y, delta=delta, prior=prior )
  if( is.na(temp) ){ temp <- 0 }
  if( Y == 0 ){ STOP <- TRUE }
  if( temp > theta ){ STOP <- TRUE }
 }

 if( temp > theta ){ out <- Y 
 } else{ out <- NA }

 return( c(j = j, out = out) )
}

Max <- 2000
N.tab <- expand.grid( list( seq(100,1500,100), seq(100,1500,100) ) )
N.tab <- N.tab[-which( rowSums(N.tab) > Max ), ]

Tab <- as.data.frame( list(n0=NA, n1=NA, y0=NA) ) 
for(i in 1:nrow(N.tab)){

 s <- 0:N.tab[i,1]
 temp <- as.data.frame( list(n0=rep(N.tab[i,1],length(s)),
                             n1=rep(N.tab[i,2],length(s)),
                             y0=s) )
 Tab <- rbind(Tab, temp)
}
Tab <- Tab[-1,]
 

 prior <- c(0.1,0.9)
 delta <- 0
 theta <- 0.9516

Tab2 <- Tab[1:101, ]

temp <- 
  1:nrow(Tab2) %>% 
  map_df(~find.Boundary(j = .x, theta = 0.9516, Tab = Tab2, delta = 0, 
                     prior = c(0.1,0.9)))

 ### Merge Boundaries ###
 Tab2$y1.stop <- rep(NA, nrow(Tab2) )
 Tab2$y1.stop[ temp[[1]] ] <- temp[[2]]
```

