---
title: "One-sample expansion cohort"
author: "Emily C. Zabor"
date: "Last updated: `r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc_depth: 3
vignette: >
  %\VignetteIndexEntry{One-sample expansion cohort}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


# Introduction

The goal of this article is to provide a tutorial on the use of the functions from {ppseq} in the context of a one-sample expansion cohort. It is becoming increasingly common for expansion cohorts to be included at the end of phase I trials. An expansion cohort is a group of additional patients being treated at the recommended phase 2 dose (RP2D) that was identified in the dose finding portion of the phase I study, and is typically used to further characterize the toxicity and/or efficacy of the RP2D. Since one goal of expansion cohorts is to conserve the limited available resources for clinical studies, early stopping for futility is an important consideration. In this article we demonstrate the use of sequential posterior probability monitoring for futility in the context of a one-sample expansion cohort.


# Setup

To execute the code in this tutorial, you first need to install and load the {ppseq} package.

```{r setup}
# remotes::install_github("zabore/ppseq")
library(ppseq)
```


# Trial design

We want to plan a one-sample expansion study. The unacceptable response rate is 0.1, and the acceptable response rate is 0.3. We plan to check for futility after every 5 patients, with a maximum sample size of 25.


## Single simulated trial

Let's start by seeing what happens in a single simulated trial. We use the function `sim_single_trial()` to simulate response counts based on the properties of our trial and calculate the posterior predictive value of exceeding the target posterior probability of success at each look. For simplicity, let's say that we would stop for futility if our posterior predictive probability of exceeding the target posterior probability, which in this case is 0.95, falls below 0.1. 

First let's consider a trial where we achieve our acceptable response rate of 0.3.

```{r}
set.seed(123)

alt1_tbl <-
  sim_single_trial(prob = 0.3, n = seq(5, 50, 5), direction = "greater", 
                   p = 0.1, delta = NULL, prior = c(0.5,0.5), S = 5000, 
                   N = 50, theta = 0.95)
```

```{r echo = FALSE}
gt::gt(alt1_tbl)
```

We see here that the posterior predictive probability, in the column `ppp`, never drops below the futility level of 0.1, so we would continue enrolling in this expansion cohort until the end of the trial, where we see we have reached a 100\% predictive probability of having posterior probability exceeding 0.95. 

Now let's consider a trial where we do not achieve our acceptable response rate of 0.3, but rather observe the unacceptable response rate of 0.1.

```{r}
set.seed(123)

null1_tbl <- 
  sim_single_trial(prob = 0.1, n = seq(5, 50, 5), direction = "greater", 
                   p = 0.1, delta = NULL, prior = c(0.5,0.5), S = 5000, 
                   N = 50, theta = 0.95)
```

```{r echo = FALSE}
gt::gt(null1_tbl)
```

We see in this example that we drop below the futility threshold of 0.1 after enrolling 15 patients, at which point we have a posterior predictive probability of 9.7\% of exceeding the posterior probability of 0.95. This would cause the trial to stop for futility to preserve resources for the investigation of more promising treatments.


## Trial operating characteristics

But when we are designing a trial, we want to achieve fixed operating characteristics such as type I error and power. There are several goals:

1. Determine the posterior probability threshold to declare the trial positive. For demonstration purposes in the previous examples we used an arbitrary threshold of 0.95, but in practice we would want to calibrate this threshold to the desired design properties of our trial.
2. Determine the predictive probability threshold to stop for futility, calibrated to achieve the desired design properties of our trial.
3. Using the identified thresholds, simulate many trials to determine the average probability of stopping early for futility. 
4. Consider different interim monitoring schedules


### Calibrating posterior threshold

To accomplish the first goal, we simulate many trials and search over a range of thresholds to identify the threshold that achieves our desired design properties

The `calibrate_posterior_threshold()` function can accomplish this at the final sample size only, not considering any interim looks or early stopping.

We can calibrate for type I error by running this simulation in the null setting where both our target and observed response probabilities are set to 0.1. We look over a range of posterior thresholds.

```{r}
set.seed(123)

thresholds <- c(0, 0.7, 0.74, 0.78, 0.82, 0.86, 0.9, 0.92, 0.93, 0.94, 0.95,
                0.96, 0.97, 0.98, 0.99, 0.999, 0.9999, 0.99999, 1)

post_tbl <- 
  calibrate_posterior_threshold(prob = 0.1, N = 25, direction = "greater", 
                                p0 = 0.1, delta = NULL, prior = c(0.5,0.5), 
                                S = 5000, theta = thresholds)
```

```{r echo = FALSE}
gt::gt(post_tbl)
```

The resulting table contains the posterior probability threshold in the column `pp_threshold` and the proportion of positive trials in the column `prop_pos`. The proportion of positive trials will be the type I error in a null setting and the power in an alternative setting. In this case, we see that if we are targeting a type I error of 0.1, this falls between a threshold of 0.82 and 0.86. We can further refine our target thresholds and recalculate.

```{r}
set.seed(123)

thresholds <- seq(0.82, 0.86, 0.001)

post_tbl <- 
  calibrate_posterior_threshold(prob = 0.1, N = 25, direction = "greater", 
                                p0 = 0.1, delta = NULL, prior = c(0.5,0.5), 
                                S = 5000, theta = thresholds)
```

```{r echo = FALSE}
gt::gt(post_tbl[post_tbl$pp_threshold <= 0.855 & 
                  post_tbl$pp_threshold >= 0.850, ])
```

And we see that a threshold of 0.853 results in a type I error rate of 0.0990, just below our desired target of 0.1. (*Note that I have printed only a subset of the resulting table above*) 


### Simulataneously calibrating posterior and predictive probabilities

However, our real goal in this case is to simultaneously calibrate the posterior threshold and the predictive threshold, considering the interim looks for futility. The `calibrate_thresholds()` function can accomplish this, and will return the average sample size, the proportion of positive trials, and the proportion of trials that were stopped early for each combination of considered thresholds, under both the null and alternative settings, thus accomplishing goals 1 through 3 as outlined above. The function would have to be run multiple times with different inputs in order to accomplish goal 4 of comparing interim monitoring schedules.

We consider the null scenario where the target response probability is 0.1 and the observed response probability is also 0.1, and the alternative scenario where the observed response probability is 0.3. We generate 1000 simulated datasets. This function is written using the `future` and `furrr` packages, but the user will have to set up a call to `future::plan` that is appropriate for their operating environment prior to running the function. I used the following code on a Unix server with 192 cores, with the goal of utilizing a maximum of 90 cores or leaving at least one core available. 

```{r eval = FALSE}
set.seed(123)

no_cores <- min(parallelly::availableCores() - 1, 90)
future::plan(future::multicore(workers = no_cores))

cal_tbl <- 
  calibrate_thresholds(prob_null = 0.1, prob_alt = 0.3, n = seq(5, 25, 5),
                       direction = "greater", p0 = 0.1, delta = NULL, 
                       prior = c(0.5, 0.5), S = 5000, N = 25, nsim = 1000,
                       pp_threshold = c(0, 0.7, 0.74, 0.78, 0.82, 0.86, 0.9, 
                                        0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 
                                        0.98, 0.99, 0.999, 0.9999, 0.99999, 1),
                       ppp_threshold = seq(0.05, 0.2, 0.05))
```

We can print the results with a call to print, and can limit the printed results to those within a desired range of acceptable type I error and meeting a desired minimum power.

```{r eval = FALSE}
print(cal_tbl, type1_range = c(0.05, 0.1), minimum_power = 0.7)
```

```{r echo = FALSE}
gt::gt(dplyr::filter(cal_tbl$res_summary, prop_pos_null >= 0.05, 
                     prop_pos_null <= 0.1, prop_pos_alt >= 0.7))
```

In this table we have considered 19 different posterior probability thresholds, and 4 different predictive probability thresholds, but here we only print the results that produced an acceptable type I error between 0.05 and 0.1 and a power of at least 70\%. The column labeled `prop_pos_null` represents the type I error rate and the column labeled `prop_pos_alt` represents the power. We see that within our acceptable range of type I error and power, we get results with power ranging from 0.761 to 0.893. These are all fairly reasonable values of power to achieve in a phase I trial.  
But which design should we choose? 

We can plot the results to visualize the different design options.

```{r eval = FALSE}
plot(cal_tbl, type1_range = c(0.05, 0.1), minimum_power = 0.7)
```

```{r echo = FALSE}
# Setup data for plotting
x <- cal_tbl
type1_range <- c(0.05, 0.1)
minimum_power <- 0.7

plot_x <- 
  dplyr::rename(
    dplyr::mutate(
      dplyr::filter(
        x$res_summary, 
        prop_pos_null >= type1_range[1] & 
          prop_pos_null <= type1_range[2] &
          prop_pos_alt >= minimum_power
        ),
      `Youden's index` = prop_pos_alt + prop_stopped_null - 1,
      Design = paste0("Posterior threshold ", pp_threshold, 
                      " and predictive threshold ", ppp_threshold),
      n_dist_metric = ((mean_n1_null - min(mean_n1_null))^2 + 
                         (mean_n1_alt - max(mean_n1_alt))^2)^(1/2)
      ),
    `Type I error` = prop_pos_null,
    Power = prop_pos_alt,
    `Average N under the null` = mean_n1_null,
    `Average N under the alternative` = mean_n1_alt,
    `Distance to min(N under null) and max(N under alt)` = n_dist_metric
    )
```

```{r echo = FALSE, fig.width = 8, fig.height = 6}
# Plot 1
p1 <- 
  ggplot2::ggplot(plot_x, 
                  ggplot2::aes(
                    x = `Type I error`, 
                    y = Power, 
                    color = `Youden's index`,
                    Design = Design)) + 
  ggplot2::geom_point() + 
  ggplot2::ylim(0, 1) + 
  ggplot2::xlim(0, 1) +
  ggplot2::labs(
    x = "Type I error",
    y = "Power"
    ) +
  ggplot2::scale_color_viridis_c() +
  ggplot2::theme_bw()
  
plotly::ggplotly(p1)
```

```{r echo = FALSE, fig.width = 8, fig.height = 6}
# Plot 2
p2 <- 
  ggplot2::ggplot(plot_x, 
                  ggplot2::aes(
                    x = `Average N under the null`, 
                    y = `Average N under the alternative`,
                    color = `Distance to min(N under null) and max(N under alt)`,
                    Design = Design)) + 
  ggplot2::geom_point() + 
  ggplot2::ylim(min(plot_x$`Average N under the null`), 
                max(plot_x$`Average N under the alternative`)) +
  ggplot2::xlim(min(plot_x$`Average N under the null`), 
                max(plot_x$`Average N under the alternative`)) +
  ggplot2::labs(
    x = "Average N under the null",
    y = "Average N under the alternative",
    color = "Distance to best"
  ) +
  ggplot2::scale_color_viridis_c() +
  ggplot2::theme_bw()

plotly::ggplotly(p2)
```

Both of these plots suggest selecting the design with posterior probability threshold 0.93 and predictive probability threshold 0.1, which maximizes both Youden's index and the distance to the point with minimum average sample size under the null and maximum average sample size under the alternative. This design has a type I error rate of 0.08, power of 89.3\%, average sample size under the null of 17, and average sample size under the alternative of 25.

The function `optimize_design` will return the design options that maximize sensitivity, specificity, and Youden's index within the specified range of acceptable type I error and minimum power. Based on the clinical context of the trial design, one of these designs can be selected for use.

```{r}
optimize_design(cal_tbl, type1_range = c(0.05, 0.1), minimum_power = 0.7)
```

